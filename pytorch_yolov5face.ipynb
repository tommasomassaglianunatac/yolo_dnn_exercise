{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from face_detector import YoloFaceDetector\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "c:\\Bigdata\\yolo_dnn_exercise\\weights/yolov5n_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "model = YoloFaceDetector(target_size=720,gpu=-1,min_face=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[227, 85, 401, 317]]]\n",
      "[[[228, 84, 402, 318]]]\n",
      "[[[228, 78, 405, 322]]]\n",
      "[[[228, 85, 404, 319]]]\n",
      "[[[228, 83, 403, 324]]]\n",
      "[[[227, 83, 402, 318]]]\n",
      "[[[229, 82, 404, 318]]]\n",
      "[[[230, 84, 405, 322]]]\n",
      "[[[235, 82, 412, 323]]]\n",
      "[[[237, 86, 413, 322]]]\n",
      "[[[237, 84, 412, 323]]]\n",
      "[[[236, 80, 412, 321]]]\n",
      "[[[236, 85, 412, 321]]]\n",
      "[[[235, 80, 413, 322]]]\n",
      "[[[237, 83, 414, 322]]]\n",
      "[[[258, 78, 433, 311]]]\n",
      "[[[283, 82, 458, 317]]]\n",
      "[[[14, 250, 91, 346], [321, 79, 503, 327]]]\n",
      "[[[336, 90, 514, 327]]]\n",
      "[[[348, 86, 522, 319]]]\n",
      "[[[351, 88, 522, 320]]]\n",
      "[[[349, 92, 521, 324]]]\n",
      "[[[349, 87, 522, 323]]]\n",
      "[[[348, 87, 522, 320]]]\n",
      "[[[349, 91, 522, 324]]]\n",
      "[[[351, 91, 523, 324]]]\n",
      "[[[352, 93, 523, 324]]]\n",
      "[[[351, 90, 523, 324]]]\n",
      "[[[350, 91, 522, 324]]]\n",
      "[[[350, 90, 523, 325]]]\n",
      "[[[349, 87, 523, 323]]]\n",
      "[[[349, 91, 522, 324]]]\n",
      "[[[349, 89, 522, 321]]]\n",
      "[[[349, 89, 523, 322]]]\n",
      "[[[348, 91, 521, 324]]]\n",
      "[[[348, 92, 520, 322]]]\n",
      "[[[347, 91, 520, 324]]]\n",
      "[[[346, 90, 520, 326]]]\n",
      "[[[347, 91, 519, 325]]]\n",
      "[[[346, 90, 520, 326]]]\n",
      "[[[346, 89, 520, 325]]]\n",
      "[[[345, 93, 519, 327]]]\n",
      "[[[345, 94, 519, 324]]]\n",
      "[[[345, 92, 519, 327]]]\n",
      "[[[342, 92, 519, 328]]]\n",
      "[[[342, 88, 519, 330]]]\n",
      "[[[341, 90, 518, 332]]]\n",
      "[[[338, 94, 516, 324], [117, 187, 195, 281]]]\n",
      "[[[141, 162, 237, 276], [337, 88, 515, 318]]]\n",
      "[[[169, 153, 269, 277], [337, 98, 516, 318]]]\n",
      "[[[190, 153, 291, 275], [337, 85, 518, 318]]]\n",
      "[[[196, 161, 299, 287], [337, 90, 517, 318]]]\n",
      "[[[190, 166, 293, 292], [334, 88, 515, 323]]]\n",
      "[[[183, 168, 283, 290], [333, 94, 512, 323]]]\n",
      "[[[177, 165, 276, 287], [332, 94, 512, 324]]]\n",
      "[[[167, 163, 267, 284], [332, 92, 511, 327]]]\n",
      "[[[146, 162, 246, 282], [333, 92, 513, 325]]]\n",
      "[[[137, 164, 233, 281], [332, 89, 512, 320]]]\n",
      "[[[120, 169, 217, 286], [332, 87, 512, 320]]]\n",
      "[[[117, 169, 215, 290], [332, 88, 511, 319]]]\n",
      "[[[116, 171, 213, 289], [331, 87, 510, 320]]]\n",
      "[[[117, 172, 215, 291], [332, 88, 510, 320]]]\n",
      "[[[106, 178, 202, 308], [332, 85, 511, 323]]]\n",
      "[[[332, 88, 507, 315]]]\n",
      "[[[322, 83, 498, 311]]]\n",
      "[[[277, 68, 455, 290]]]\n",
      "[[[241, 57, 422, 277]]]\n",
      "[[[210, 50, 392, 269]]]\n",
      "[[[176, 53, 356, 276]]]\n",
      "[[[170, 52, 351, 281]]]\n",
      "[[[168, 56, 349, 285]]]\n",
      "[[[174, 69, 350, 287]]]\n",
      "[[[183, 68, 359, 284]]]\n",
      "[[[204, 67, 381, 291]]]\n",
      "[[[208, 68, 388, 305]]]\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    _, img = cap.read()\n",
    "    \n",
    "    bboxes,points = model.predict(img)\n",
    "    print(bboxes)\n",
    "    \n",
    "    for el in bboxes:\n",
    "        for bbox in el:\n",
    "            cv2.rectangle(img, (bbox[0],bbox[1]),(bbox[2],bbox[3]),(255,0,0),1)\n",
    "        \n",
    "    cv2.imshow(\"Webcam\", img) # This will open an independent window\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF==ord('q'): # quit when 'q' is pressed\n",
    "        cap.release()\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2cb6097e1f2020c64c91be10047db92b26d9d39ef349dc004c94f62fe0e20deb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
